{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPJ+aA+fgRUR7KIE/7ZsIUI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TOM-BOHN/MsDS-marketing-network-analysis/blob/main/marketing_network_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Marketing Network Analysis\n",
        "**Thomas Bohn**   --   **2023-10-25**\n",
        "\n",
        "A report focused on Network Analysis using [nltk](https://www.nltk.org/) to process text from twitter and [NetworkX](https://networkx.org/) to analyze the network. The network analysis will focus 2 types of networks, a user relationship network, and a semantic network. The user relationship network will focus on understanding users central to the brands and user influential in the product category, focusing on the analysis of twitter mention as a graph data structure. The semantic network anlysis will focus on better understanding the conversation around each brand, what makes the brand unique, and what makes each brand different.\n",
        "\n",
        "--  [Main Report](tbd)  --  [Github Repo](tbd)  --  [Presentation Slides](tbd)  --  [Presentation Video](tbd) --"
      ],
      "metadata": {
        "id": "vea2LnX4oFBA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.&nbsp;Introduction"
      ],
      "metadata": {
        "id": "Qv1t50XHoFKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Context**\n",
        "Network Analysis is applicable to any dataset where the relationships between elements are importnant. One source of rich network (or graph) data is through Twitter or other social networks, where people post, share, and mentions other.In the case of network analysis, we can use tweets that are created and mention brands of interest to understand the conversation around a specific topic or category.\n",
        "\n",
        "**Background**\n",
        "This notebook focuses on an analysis of how consumers talk about three competing brands: **Nike**, **Adidas**, and **Lululemon**. This will take the form of network analysis and semantic network analysis, using a graph data representation of the dataset to look for trends and patterns.\n",
        "\n",
        "The goal is to understand the chatter around each brand better, what makes each brand unique, and what makes each brand different. Network analysis will also be used to identify users that are most central to the brand and users that are hyper interested in the product category of athletic wear.\n",
        "\n",
        "**Data Source**\n",
        "The dataset is sourced from the Twitter API, specifically the [Search Endpoint](https://developer.twitter.com/en/docs/twitter-api/v1/tweets/search/api-reference/get-search-tweets) to retrieve Tweets that mention the three brands of interest. The following describes the scope of the data:\n",
        "- Tweets were retrieved over the last 93 days\n",
        "- About ~150k tweets are included in the dataset\n",
        "- Tweets are “at Mentions” (@nike, @lululemon, @adidas)\n",
        "- Tweets were sent from the US and are in English\n",
        "\n",
        "The raw data sources for the project can be accessed with the following links:\n",
        "- [Tweet Data](http://128.138.93.164/nikelululemonadidas_tweets.jsonl.gz)\n",
        "\n",
        "**Overview of Observations**\n",
        "\n",
        "\n",
        "\n",
        "**Objective**\n",
        "The objective is to build a unsupervize network analysis to explore and analyze the dataset for 3 athletic wear brands: **Nike**, **Adidas**, and **Lululemon**. The analysis wil follow 3 worksteams:\n",
        "1. **Twitter Mentions Graph** - A valued, directed network graph of Twitter mentions. Will show Twitter users that are most centrally related to the brand (e.g., they regularly mention the brand). This graph will also illustrate who mentions who on Twitter, and in what way those mentions flow. One mention graph will be created with mentions for all three brands.\n",
        "2. **Semantic Network Graph** - A semantic network analysis graph of words used in Tweets. This graph will reveal what words are most commonly associated with each other, for each brand. One semantic graph will be created, with data for all three brands.\n",
        "3. **Using the Graph Data for Analysis:** Using the dataset and graphs, analyze specific questions related to the brands to help the brands understand the conversation and who is involved in the conversation.\n",
        "\n",
        "**Report Overview**\n",
        "The project will cover 5 key phases:\n",
        "1. Data Source: Extracting, filtering, and focusing the data on the Nike brand\n",
        "2. Preprocessing: Extracting Features from Tweets\n",
        "3. ...\n",
        "4. ...\n",
        "5. ...\n",
        "6. ...\n",
        "7. Data Analysis and Exploration: Answer specific questions about the graph network datasets"
      ],
      "metadata": {
        "id": "a2lQwh4pszOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Python Libraries\n",
        "\n",
        "The following python libraries are used in this notebook."
      ],
      "metadata": {
        "id": "u5twBZEioFNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('[-] Importing packages...')\n",
        "# File Connection and File Manipulation\n",
        "import os\n",
        "import pickle\n",
        "import gzip\n",
        "import json\n",
        "# Basic Data Science Toolkits\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import time\n",
        "import re\n",
        "import itertools\n",
        "import datetime\n",
        "# Basic Data Vizualization\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "# Text Preprocessing(other)\n",
        "import string\n",
        "import nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9VoR3Frq144",
        "outputId": "87635b84-a16a-4250-cce9-7b82f856648c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-] Importing packages...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the `punkt`, `stopwords` and `wordnet` datasets from nltk in our analysis. Downloaded here for use in our notebook."
      ],
      "metadata": {
        "id": "F64I9WoBrmRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download required corpus based data to nltk package\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B8x-0C1rmZF",
        "outputId": "89814feb-7c5e-485f-d364-70405bbff06a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set Global Variables"
      ],
      "metadata": {
        "id": "VSOI9ZX_oFQe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gDEBUG = True"
      ],
      "metadata": {
        "id": "AAnzeFftq1iU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Verify GPU Runtime"
      ],
      "metadata": {
        "id": "1BYH3I3DoFTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#see the GPU assigned\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gj7g2YkGqetW",
        "outputId": "32e21909-374a-42b8-db1c-005f801f580a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nvidia-smi: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#See the virtual memory assigned\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('  [.] Your runtime has {:.1f} gigabytes of available RAM'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('  [.] Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('  [.] You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk9sN070qe0i",
        "outputId": "0be280a2-a98c-4cf2-9d8e-9e60d7cec911"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  [.] Your runtime has 13.6 gigabytes of available RAM\n",
            "  [.] Not using a high-RAM runtime\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "EoXy4dDOqhzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6a82wN0qh6_",
        "outputId": "9374856e-954f-4072-94e7-65428bd0f657"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup Directories"
      ],
      "metadata": {
        "id": "e6rPLY7Iqmfn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup Directories\n",
        "ROOT_DIR = \"/content/drive/MyDrive/MSDS_marketing_text_analytics/master_files/3_network_analysis\"\n",
        "DATA_DIR = \"%s/data\" % ROOT_DIR\n",
        "EVAL_DIR = \"%s/evaluation\" % ROOT_DIR\n",
        "MODEL_DIR = \"%s/models\" % ROOT_DIR\n",
        "\n",
        "#Create missing directories, if they don't exist\n",
        "if not os.path.exists(DATA_DIR):\n",
        "  # Create a new directory because it does not exist\n",
        "  os.makedirs(DATA_DIR)\n",
        "  print(\"The data directory is created!\")\n",
        "if not os.path.exists(EVAL_DIR):\n",
        "  # Create a new directory because it does not exist\n",
        "  os.makedirs(EVAL_DIR)\n",
        "  print(\"The evaluation directory is created!\")\n",
        "if not os.path.exists(MODEL_DIR):\n",
        "  # Create a new directory because it does not exist\n",
        "  os.makedirs(MODEL_DIR)\n",
        "  print(\"The model directory is created!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdvmS9gXqm6D",
        "outputId": "834b9f70-edc2-4a67-c4c9-a586369c5c70"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The data directory is created!\n",
            "The evaluation directory is created!\n",
            "The model directory is created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.&nbsp;Data Source\n",
        "\n",
        "Import and process the Twitter data."
      ],
      "metadata": {
        "id": "a1BMdG-sst3Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Copy Data From Source"
      ],
      "metadata": {
        "id": "H_1s3y--JSQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Copy Data From Source\n",
        "#!wget <URL> -P <COLAB PATH>\n",
        "#source_url = 'http://128.138.93.164/nikelululemonadidas_tweets.jsonl.gz' # true source, need better link\n",
        "source_url = 'https://docs.google.com/uc?export=download&id=12sq73UTafhP6M8iUuP62yr4uRthFEp_-&confirm=t' # local source, working for testing\n",
        "dest_path = '%s/nikelululemonadidas_tweets.jsonl.gz' % DATA_DIR\n",
        "!wget \"$source_url\" -O \"$dest_path\""
      ],
      "metadata": {
        "id": "cAGcm2gxJRQ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_file_path = '%s/nikelululemonadidas_tweets.jsonl.gz' % DATA_DIR\n",
        "!gzip -d \"$tweet_file_path\""
      ],
      "metadata": {
        "id": "RH_M-2d5KQmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inspect Some of the Data"
      ],
      "metadata": {
        "id": "berzG_GDKyGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "LIMIT = 5\n",
        "tweet_file_path = '%s/nikelululemonadidas_tweets.jsonl' % DATA_DIR\n",
        "\n",
        "# Inspect LIMIT number of Tweets that mention Nike\n",
        "with gzip.open(tweet_file_path) as data_file:\n",
        "    for i, line in enumerate(data_file):\n",
        "        if i >= LIMIT:\n",
        "            break\n",
        "        tweet = json.loads(line)\n",
        "        text = tweet.get(\"full_text\") or tweet.get(\"text\")\n",
        "        if \"nike\" in text.lower():\n",
        "            print(text)"
      ],
      "metadata": {
        "id": "oJ2AtGYZKvjX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load the Product Data\n",
        "##this assigns the filename we're trying to load in to a string variable\n",
        "tweet_file_path = '%s/nikelululemonadidas_tweets.jsonl' % DATA_DIR\n",
        "loadedjson = open(meta_file_path, 'r')"
      ],
      "metadata": {
        "id": "zhc_i0hiKcF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Product Data"
      ],
      "metadata": {
        "id": "ufxzGfZcKc76"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.&nbsp;Create a Mention Network"
      ],
      "metadata": {
        "id": "rSBUxu13LPxg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify unique users in the mention network\n",
        "users = {}\n",
        "tweet_file_path = '%s/nikelululemonadidas_tweets.jsonl' % DATA_DIR\n",
        "\n",
        "with gzip.open(tweet_file_path) as data_file:\n",
        "    for i, line in enumerate(data_file):\n",
        "        if i % 10000 == 0: # Show a periodic status\n",
        "            print(\"%s tweets processed\" % i)\n",
        "        tweet = json.loads(line)\n",
        "        user = tweet[\"user\"]\n",
        "        user_id = user[\"id\"]\n",
        "        if user_id not in users:\n",
        "            users[user_id] = {\n",
        "                \"id\": user_id,\n",
        "                \"tweet_count\": 0,\n",
        "                \"followers_count\": user[\"followers_count\"]\n",
        "            }\n",
        "        users[user_id][\"tweet_count\"] += 1\n",
        "    print(f\"{i} total tweets processed\")"
      ],
      "metadata": {
        "id": "Z-GSlB0_LFUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('there are', len(users), 'total users in the mention network.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "3hpL2-87LiSa",
        "outputId": "8fd9de6a-1a44-4b98-92fd-c7dc6ae5c908"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-854f65a559c4>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0musers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'users' is not defined"
          ]
        }
      ]
    }
  ]
}